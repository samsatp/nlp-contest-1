{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543ae626",
   "metadata": {},
   "source": [
    "# $\\text{Import data}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4406d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3156, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>aspectCategory</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3121</td>\n",
       "      <td>But the staff was so horrible to us.</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2777</td>\n",
       "      <td>To be completely fair, the only redeeming fact...</td>\n",
       "      <td>anecdotes/miscellaneous</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1634</td>\n",
       "      <td>The food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2534</td>\n",
       "      <td>Where Gabriela personaly greets you and recomm...</td>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  \\\n",
       "0  3121               But the staff was so horrible to us.   \n",
       "1  2777  To be completely fair, the only redeeming fact...   \n",
       "2  2777  To be completely fair, the only redeeming fact...   \n",
       "3  1634  The food is uniformly exceptional, with a very...   \n",
       "4  2534  Where Gabriela personaly greets you and recomm...   \n",
       "\n",
       "            aspectCategory  polarity  \n",
       "0                  service  negative  \n",
       "1                     food  positive  \n",
       "2  anecdotes/miscellaneous  negative  \n",
       "3                     food  positive  \n",
       "4                  service  positive  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"contest1_train.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9cebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dups(df, name:str):\n",
    "    if any(df.duplicated()):\n",
    "        dups = df[df.duplicated()]\n",
    "        return df.drop_duplicates()\n",
    "    \n",
    "df = drop_dups(df, \"Whole data\")\n",
    "\n",
    "df_aspect = df[['text', 'aspectCategory']]\n",
    "df_sentiment = df[['text', 'polarity']]\n",
    "\n",
    "df_aspect = drop_dups(df_aspect, \"aspect\")\n",
    "df_sentiment = drop_dups(df_sentiment, \"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd878044",
   "metadata": {},
   "source": [
    "# $\\text{1. Sentiment}$\n",
    "- Rule-based\n",
    "- BOW\n",
    "- TF-IDF\n",
    "- Bidirectional GRU\n",
    "- CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de250e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_models import utils, sentiment\n",
    "\n",
    "# Drop texts that are duplicated\n",
    "df_sentiment = df_sentiment.drop_duplicates(subset=['text'], keep='last')\n",
    "\n",
    "X_TRAIN_sent, X_DEV_sent, Y_TRAIN_sent, Y_DEV_sent = utils.split_data(df_sentiment['text'], df_sentiment['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c8e75",
   "metadata": {},
   "source": [
    "## 1.1) Rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8e0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.50      0.01      0.01       148\n",
      "    negative       0.79      0.03      0.06       602\n",
      "     neutral       0.15      0.98      0.26       354\n",
      "    positive       0.94      0.14      0.24      1478\n",
      "\n",
      "    accuracy                           0.22      2582\n",
      "   macro avg       0.60      0.29      0.14      2582\n",
      "weighted avg       0.77      0.22      0.19      2582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = sentiment.VADER(df_sentiment['text'])\n",
    "utils.get_reports(y_true = df_sentiment['polarity'], y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0828e9",
   "metadata": {},
   "source": [
    "## 1.2) Logistic regression (bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6714c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new BOW vectorizer...\n",
      "BOW matrix: (2065, 3589)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.43      0.30      0.35        30\n",
      "    negative       0.63      0.53      0.58       120\n",
      "     neutral       0.54      0.27      0.36        71\n",
      "    positive       0.73      0.89      0.80       296\n",
      "\n",
      "    accuracy                           0.68       517\n",
      "   macro avg       0.58      0.50      0.52       517\n",
      "weighted avg       0.66      0.68      0.66       517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_sent_bow = sentiment.LOGREG(feature_mode=\"BOW\", max_iter=200)\n",
    "# preprocess\n",
    "X_train_sent = logreg_sent_bow.preprocess(X_TRAIN_sent.values)\n",
    "X_dev_sent = logreg_sent_bow.preprocess(X_DEV_sent.values)\n",
    "\n",
    "# train\n",
    "logreg_sent_bow.fit(X_train_sent, Y_TRAIN_sent)\n",
    "\n",
    "# inference\n",
    "y_pred = logreg_sent_bow.predict(X_dev_sent)\n",
    "utils.get_reports(y_true = Y_DEV_sent, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1d6db",
   "metadata": {},
   "source": [
    "## 1.3) Logistic regression (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a89430a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new TFIDF vectorizer...\n",
      "TFIDF matrix: (2065, 3589)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.00      0.00      0.00        30\n",
      "    negative       0.66      0.42      0.52       120\n",
      "     neutral       0.91      0.14      0.24        71\n",
      "    positive       0.67      0.96      0.79       296\n",
      "\n",
      "    accuracy                           0.67       517\n",
      "   macro avg       0.56      0.38      0.39       517\n",
      "weighted avg       0.66      0.67      0.61       517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_sent_tfidf = sentiment.LOGREG(feature_mode=\"TFIDF\", max_iter=200)\n",
    "# preprocess\n",
    "X_train_sent = logreg_sent_tfidf.preprocess(X_TRAIN_sent.values)\n",
    "X_dev_sent = logreg_sent_tfidf.preprocess(X_DEV_sent.values)\n",
    "\n",
    "# train\n",
    "logreg_sent_tfidf.fit(X_train_sent, Y_TRAIN_sent)\n",
    "\n",
    "# inference\n",
    "y_pred = logreg_sent_tfidf.predict(X_dev_sent)\n",
    "utils.get_reports(y_true = Y_DEV_sent, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8bb6b",
   "metadata": {},
   "source": [
    "## 1.4) Bidirectional GRU (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c00d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "vocab, embedding_matrix = utils.get_embeddings(emb_dim)\n",
    "\n",
    "maxlen = 30\n",
    "vocab_size = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ef29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_models.inference import get_rnn, get_cnn\n",
    "\n",
    "rnn_params = dict(\n",
    "    rnn_layers=[128,128], \n",
    "    dense_layers=[64,64], \n",
    "    embedding_matrix=embedding_matrix, \n",
    "    n_outputs=len(Y_TRAIN_sent.unique()), \n",
    "    embedding_trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ade884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_sent = sentiment.dl_glove(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b88d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained word embedding\n",
      "cloning model from template...\n"
     ]
    }
   ],
   "source": [
    "## Reinstantiate model\n",
    "rnn_sent.set_model_template(get_rnn(**rnn_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fa61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Build new LabelEncoder\n"
     ]
    }
   ],
   "source": [
    "X_train_sent, Y_train_sent = rnn_sent.preprocess(X_TRAIN_sent.values, Y_TRAIN_sent.values, maxlen=30)\n",
    "X_dev_sent, Y_dev_sent = rnn_sent.preprocess(X_DEV_sent.values, Y_DEV_sent.values, maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a438a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['conflict', 'negative', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_sent.le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcff1175",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "12/65 [====>.........................] - ETA: 1:22 - loss: 1.1378 - accuracy: 0.5547"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_rnn_sent \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_sent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_sent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_sent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_dev_sent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_dev_sent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\my_models\\sentiment.py:185\u001b[0m, in \u001b[0;36mdl_glove.fit\u001b[1;34m(self, X_train, Y_train, X_dev, Y_dev, batch_size, epochs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# preprocessed datasets*\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     X_train, Y_train, \n\u001b[0;32m    182\u001b[0m     X_dev, Y_dev, \n\u001b[0;32m    183\u001b[0m     batch_size, epochs\n\u001b[0;32m    184\u001b[0m ):\n\u001b[1;32m--> 185\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_dev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_rnn_sent = rnn_sent.fit(\n",
    "    X_train_sent, Y_train_sent, X_dev_sent, Y_dev_sent,\n",
    "    batch_size = 32, epochs = 6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c6928e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.25      0.23      0.24        30\n",
      "    negative       0.51      0.53      0.52       120\n",
      "     neutral       0.34      0.38      0.36        71\n",
      "    positive       0.78      0.75      0.77       296\n",
      "\n",
      "    accuracy                           0.62       517\n",
      "   macro avg       0.47      0.47      0.47       517\n",
      "weighted avg       0.63      0.62      0.62       517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rnn_sent.predict(X_dev_sent)\n",
    "utils.get_reports(\n",
    "    y_true = [rnn_sent.le.classes_[i] for i in Y_dev_sent], \n",
    "    y_pred= [rnn_sent.le.classes_[i] for i in y_pred]\n",
    ") # trainable embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edd2a9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.16      0.10      0.12        30\n",
      "    negative       0.50      0.45      0.48       120\n",
      "     neutral       0.55      0.08      0.15        71\n",
      "    positive       0.67      0.86      0.76       296\n",
      "\n",
      "    accuracy                           0.62       517\n",
      "   macro avg       0.47      0.37      0.38       517\n",
      "weighted avg       0.59      0.62      0.57       517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rnn_sent.predict(X_dev_sent)\n",
    "utils.get_reports(\n",
    "    y_true = [rnn_sent.le.classes_[i] for i in Y_dev_sent], \n",
    "    y_pred= [rnn_sent.le.classes_[i] for i in y_pred]\n",
    ") #300d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3fdf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_le = rnn_sent.le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044f51c",
   "metadata": {},
   "source": [
    "## 1.5) CNN (glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d03c61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_params = dict(\n",
    "    n_filters = 64,\n",
    "    kernel_size = 3,\n",
    "    n_cnn_layers = 3,\n",
    "    dense_layers = [64,64],\n",
    "    embedding_matrix = embedding_matrix,\n",
    "    n_outputs = len(Y_TRAIN_sent.unique()),\n",
    "    embedding_trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5927ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_sent = sentiment.dl_glove(vocab, le=global_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c454059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained word embedding\n",
      "cloning model from template...\n"
     ]
    }
   ],
   "source": [
    "cnn_sent.set_model_template(get_cnn(**cnn_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9517498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Build new LabelEncoder\n"
     ]
    }
   ],
   "source": [
    "#X_train_sent, Y_train_sent = cnn_sent.preprocess(X_TRAIN_sent.values, Y_TRAIN_sent.values, maxlen=maxlen)\n",
    "#X_dev_sent, Y_dev_sent = cnn_sent.preprocess(X_DEV_sent.values, Y_DEV_sent.values, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fba4138f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "65/65 [==============================] - 111s 1s/step - loss: 1.1917 - accuracy: 0.5327 - val_loss: 1.0864 - val_accuracy: 0.5725\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 89s 1s/step - loss: 1.0802 - accuracy: 0.5646 - val_loss: 1.0548 - val_accuracy: 0.5803\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 84s 1s/step - loss: 0.9618 - accuracy: 0.6029 - val_loss: 0.9916 - val_accuracy: 0.6190\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 85s 1s/step - loss: 0.7977 - accuracy: 0.6731 - val_loss: 1.0084 - val_accuracy: 0.6054\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 89s 1s/step - loss: 0.6330 - accuracy: 0.7327 - val_loss: 1.1033 - val_accuracy: 0.5803\n"
     ]
    }
   ],
   "source": [
    "history_cnn_sent = cnn_sent.fit(\n",
    "    X_train_sent, Y_train_sent, X_dev_sent, Y_dev_sent,\n",
    "    batch_size = 32, epochs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96c30273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.00      0.00      0.00        30\n",
      "    negative       0.38      0.78      0.51       120\n",
      "     neutral       0.00      0.00      0.00        71\n",
      "    positive       0.77      0.70      0.73       296\n",
      "\n",
      "    accuracy                           0.58       517\n",
      "   macro avg       0.29      0.37      0.31       517\n",
      "weighted avg       0.53      0.58      0.54       517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn_sent.predict(X_dev_sent)\n",
    "utils.get_reports(\n",
    "    y_true = [cnn_sent.le.classes_[i] for i in Y_dev_sent], \n",
    "    y_pred= [cnn_sent.le.classes_[i] for i in y_pred]\n",
    ") # trainable embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e6766b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    conflict       0.00      0.00      0.00        30\n",
      "    negative       0.57      0.07      0.12       120\n",
      "     neutral       0.00      0.00      0.00        71\n",
      "    positive       0.58      0.99      0.74       296\n",
      "\n",
      "    accuracy                           0.58       517\n",
      "   macro avg       0.29      0.26      0.21       517\n",
      "weighted avg       0.47      0.58      0.45       517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn_sent.predict(X_dev_sent)\n",
    "utils.get_reports(\n",
    "    y_true = [cnn_sent.le.classes_[i] for i in Y_dev_sent], \n",
    "    y_pred= [cnn_sent.le.classes_[i] for i in y_pred]\n",
    ") #non-trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e666e",
   "metadata": {},
   "source": [
    "## 1.6) BOW NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b5eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_bowNN(dense_layers):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for units in dense_layers:\n",
    "        model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(len(Y_TRAIN_sent.unique()), activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1205660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_nn = sentiment.dl(get_bowNN([512, 128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_nn.preprocess(X_TRAIN_sent.values, Y_TRAIN_sent.values, maxlen=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbdf22d",
   "metadata": {},
   "source": [
    "# $\\text{2. Aspect}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395e41bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service' 'food' 'anecdotes/miscellaneous' 'price' 'ambience']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspectCategory</th>\n",
       "      <th>service</th>\n",
       "      <th>food</th>\n",
       "      <th>anecdotes/miscellaneous</th>\n",
       "      <th>price</th>\n",
       "      <th>ambience</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>$160 for 2 filets, 2 sides, an appetizer and drinks.</th>\n",
       "      <td>[food, price]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$20 for all you can eat sushi cannot be beaten.</th>\n",
       "      <td>[price]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$20 gets you unlimited sushi of a very high quality- I even took a friend here from Japan who said it was one of the best sushi places in the US that he has been to.</th>\n",
       "      <td>[food, price]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>($200 for 2 glasses of champagne, not too expensive bottle of wine and 2 after dinner drinks).</th>\n",
       "      <td>[price]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Always ask the bartender for the SEASONAL beer!!!</th>\n",
       "      <td>[food]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   aspectCategory  service  \\\n",
       "text                                                                         \n",
       "$160 for 2 filets, 2 sides, an appetizer and dr...  [food, price]        0   \n",
       "$20 for all you can eat sushi cannot be beaten.           [price]        0   \n",
       "$20 gets you unlimited sushi of a very high qua...  [food, price]        0   \n",
       "($200 for 2 glasses of champagne, not too expen...        [price]        0   \n",
       "(Always ask the bartender for the SEASONAL beer!!!         [food]        0   \n",
       "\n",
       "                                                    food  \\\n",
       "text                                                       \n",
       "$160 for 2 filets, 2 sides, an appetizer and dr...     1   \n",
       "$20 for all you can eat sushi cannot be beaten.        0   \n",
       "$20 gets you unlimited sushi of a very high qua...     1   \n",
       "($200 for 2 glasses of champagne, not too expen...     0   \n",
       "(Always ask the bartender for the SEASONAL beer!!!     1   \n",
       "\n",
       "                                                    anecdotes/miscellaneous  \\\n",
       "text                                                                          \n",
       "$160 for 2 filets, 2 sides, an appetizer and dr...                        0   \n",
       "$20 for all you can eat sushi cannot be beaten.                           0   \n",
       "$20 gets you unlimited sushi of a very high qua...                        0   \n",
       "($200 for 2 glasses of champagne, not too expen...                        0   \n",
       "(Always ask the bartender for the SEASONAL beer!!!                        0   \n",
       "\n",
       "                                                    price  ambience  \n",
       "text                                                                 \n",
       "$160 for 2 filets, 2 sides, an appetizer and dr...      1         0  \n",
       "$20 for all you can eat sushi cannot be beaten.         1         0  \n",
       "$20 gets you unlimited sushi of a very high qua...      1         0  \n",
       "($200 for 2 glasses of champagne, not too expen...      1         0  \n",
       "(Always ask the bartender for the SEASONAL beer!!!      0         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.pivot_table(\n",
    "                df_aspect,\n",
    "                index='text',\n",
    "                values='aspectCategory',\n",
    "                aggfunc=lambda x: list(x)\n",
    "            )\n",
    "\n",
    "aspects = df_aspect.aspectCategory.unique()\n",
    "print(aspects)\n",
    "\n",
    "for a in aspects:\n",
    "    temp_df[a] = temp_df.apply(lambda x: 1 if a in x.aspectCategory else 0, axis=1)\n",
    "    \n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15363522",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_asp, X_DEV_asp, Y_TRAIN_asp, Y_DEV_asp = utils.split_data(temp_df.index, temp_df.iloc[:, -5:], stratify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62d2cd",
   "metadata": {},
   "source": [
    "## 2.1) Logistic regression (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd632e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new models\n",
      "Creating new BOW vectorizer...\n",
      "BOW matrix: (2065, 3633)\n",
      "predicting service...\n",
      "predicting food...\n",
      "predicting anecdotes/miscellaneous...\n",
      "predicting price...\n",
      "predicting ambience...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.68      0.78       106\n",
      "           1       0.88      0.79      0.84       209\n",
      "           2       0.77      0.74      0.76       191\n",
      "           3       0.88      0.54      0.67        56\n",
      "           4       0.91      0.44      0.59        73\n",
      "\n",
      "   micro avg       0.85      0.70      0.76       635\n",
      "   macro avg       0.87      0.64      0.73       635\n",
      "weighted avg       0.86      0.70      0.76       635\n",
      " samples avg       0.72      0.70      0.70       635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from my_models import aspect\n",
    "\n",
    "logreg_aspect_bow = aspect.LOGREG(feature_mode='BOW')\n",
    "X_train_asp = logreg_aspect_bow.preprocess(X_TRAIN_asp, Y_TRAIN_asp)\n",
    "X_dev_asp = logreg_aspect_bow.preprocess(X_DEV_asp, Y_DEV_asp)\n",
    "\n",
    "logreg_aspect_bow.fit(X_train_asp, Y_TRAIN_asp)\n",
    "\n",
    "outputs = logreg_aspect_bow.predict(X_dev_asp)\n",
    "\n",
    "utils.get_reports(\n",
    "    y_true = Y_DEV_asp.values, \n",
    "    y_pred= outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509ccde",
   "metadata": {},
   "source": [
    "## 2.2) Logistic regerssion (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8cce804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new models\n",
      "Creating new TFIDF vectorizer...\n",
      "TFIDF matrix: (2065, 3633)\n",
      "predicting service...\n",
      "predicting food...\n",
      "predicting anecdotes/miscellaneous...\n",
      "predicting price...\n",
      "predicting ambience...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.48      0.65       106\n",
      "           1       0.91      0.72      0.80       209\n",
      "           2       0.83      0.59      0.69       191\n",
      "           3       0.89      0.14      0.25        56\n",
      "           4       0.89      0.11      0.20        73\n",
      "\n",
      "   micro avg       0.89      0.52      0.66       635\n",
      "   macro avg       0.90      0.41      0.52       635\n",
      "weighted avg       0.90      0.52      0.62       635\n",
      " samples avg       0.59      0.55      0.56       635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from my_models import aspect\n",
    "\n",
    "logreg_aspect_tfidf = aspect.LOGREG(feature_mode='TFIDF')\n",
    "X_train_asp = logreg_aspect_tfidf.preprocess(X_TRAIN_asp, Y_TRAIN_asp)\n",
    "X_dev_asp = logreg_aspect_tfidf.preprocess(X_DEV_asp, Y_DEV_asp)\n",
    "\n",
    "logreg_aspect_tfidf.fit(X_train_asp, Y_TRAIN_asp)\n",
    "\n",
    "outputs = logreg_aspect_tfidf.predict(X_dev_asp)\n",
    "\n",
    "utils.get_reports(\n",
    "    y_true = Y_DEV_asp.values, \n",
    "    y_pred= outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8adf12",
   "metadata": {},
   "source": [
    "## 2.3) Bidirectional GRU (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15eaa694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['service', 'food', 'anecdotes/miscellaneous', 'price', 'ambience'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_TRAIN_asp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72f52440",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_params = dict(\n",
    "    rnn_layers=[128,128], \n",
    "    dense_layers=[64,64], \n",
    "    embedding_matrix=embedding_matrix, \n",
    "    n_outputs=1, \n",
    "    embedding_trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5819aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_models import aspect\n",
    "rnn_asp = aspect.dl_glove(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "901530f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained word embedding\n",
      "cloning model from template...\n"
     ]
    }
   ],
   "source": [
    "# Reinstantiate models\n",
    "rnn_asp.set_model_template(get_rnn(**rnn_params), n_models = len(Y_TRAIN_asp.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59192f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_asp, Y_train_asp = rnn_asp.preprocess(X_TRAIN_asp, Y_TRAIN_asp, maxlen=30)\n",
    "X_dev_asp, Y_dev_asp = rnn_asp.preprocess(X_DEV_asp, Y_DEV_asp, maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49dac7f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting service ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 31s 246ms/step - loss: 0.4672 - accuracy: 0.8121 - val_loss: 0.4312 - val_accuracy: 0.8337\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 9s 145ms/step - loss: 0.3394 - accuracy: 0.8678 - val_loss: 0.3217 - val_accuracy: 0.8743\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 10s 156ms/step - loss: 0.2712 - accuracy: 0.8973 - val_loss: 0.2841 - val_accuracy: 0.9033\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 10s 153ms/step - loss: 0.2314 - accuracy: 0.9128 - val_loss: 0.2889 - val_accuracy: 0.9072\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 10s 159ms/step - loss: 0.2055 - accuracy: 0.9245 - val_loss: 0.2816 - val_accuracy: 0.9033\n",
      "fitting food ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 36s 262ms/step - loss: 0.6214 - accuracy: 0.6523 - val_loss: 0.5131 - val_accuracy: 0.7447\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 9s 135ms/step - loss: 0.5221 - accuracy: 0.7438 - val_loss: 0.4509 - val_accuracy: 0.7718\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 10s 152ms/step - loss: 0.4442 - accuracy: 0.7869 - val_loss: 0.4159 - val_accuracy: 0.8046\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 9s 142ms/step - loss: 0.4121 - accuracy: 0.8107 - val_loss: 0.4213 - val_accuracy: 0.8143\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 9s 138ms/step - loss: 0.3729 - accuracy: 0.8349 - val_loss: 0.4544 - val_accuracy: 0.7814\n",
      "fitting anecdotes/miscellaneous ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 33s 296ms/step - loss: 0.5707 - accuracy: 0.6785 - val_loss: 0.5391 - val_accuracy: 0.7331\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 10s 154ms/step - loss: 0.4822 - accuracy: 0.7661 - val_loss: 0.5051 - val_accuracy: 0.7524\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 10s 149ms/step - loss: 0.4449 - accuracy: 0.7903 - val_loss: 0.4767 - val_accuracy: 0.7660\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 10s 151ms/step - loss: 0.4138 - accuracy: 0.8116 - val_loss: 0.4546 - val_accuracy: 0.7853\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 10s 149ms/step - loss: 0.3949 - accuracy: 0.8140 - val_loss: 0.4552 - val_accuracy: 0.7853\n",
      "fitting price ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 35s 278ms/step - loss: 0.3679 - accuracy: 0.8833 - val_loss: 0.3222 - val_accuracy: 0.8917\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 9s 141ms/step - loss: 0.3034 - accuracy: 0.8949 - val_loss: 0.2907 - val_accuracy: 0.9014\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 9s 141ms/step - loss: 0.2535 - accuracy: 0.9128 - val_loss: 0.2283 - val_accuracy: 0.9246\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 9s 139ms/step - loss: 0.2095 - accuracy: 0.9249 - val_loss: 0.2905 - val_accuracy: 0.9207\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 9s 142ms/step - loss: 0.1729 - accuracy: 0.9390 - val_loss: 0.2452 - val_accuracy: 0.9265\n",
      "fitting ambience ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 36s 232ms/step - loss: 0.4222 - accuracy: 0.8518 - val_loss: 0.3932 - val_accuracy: 0.8588\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 9s 143ms/step - loss: 0.3853 - accuracy: 0.8562 - val_loss: 0.3756 - val_accuracy: 0.8588\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 9s 143ms/step - loss: 0.3476 - accuracy: 0.8620 - val_loss: 0.3514 - val_accuracy: 0.8685\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 9s 146ms/step - loss: 0.3207 - accuracy: 0.8659 - val_loss: 0.3928 - val_accuracy: 0.8027\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 9s 136ms/step - loss: 0.3010 - accuracy: 0.8780 - val_loss: 0.3662 - val_accuracy: 0.8491\n"
     ]
    }
   ],
   "source": [
    "histories = rnn_asp.fit(\n",
    "                X_train_asp, Y_train_asp, X_dev_asp, Y_dev_asp,\n",
    "                batch_size = 32, epochs = 5\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_asp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "905fcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def manual_predict(X, threshold=0.5):\n",
    "    outputs = []\n",
    "    threshold = 0.5\n",
    "    for aspect, model in zip(Y_train_asp.columns, rnn_asp.models):\n",
    "        print(f'predicting {aspect}...')\n",
    "        y_pred_target = model.predict(X)\n",
    "        y_pred = tf.cast(y_pred_target > threshold, tf.int32) \n",
    "        outputs.append(y_pred.numpy().ravel())\n",
    "\n",
    "    outputs = np.transpose(np.array(outputs))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2618f421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting service...\n",
      "predicting food...\n",
      "predicting anecdotes/miscellaneous...\n",
      "predicting price...\n",
      "predicting ambience...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75       106\n",
      "           1       0.70      0.80      0.75       209\n",
      "           2       0.69      0.76      0.72       191\n",
      "           3       0.75      0.48      0.59        56\n",
      "           4       0.45      0.32      0.37        73\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       635\n",
      "   macro avg       0.68      0.61      0.64       635\n",
      "weighted avg       0.69      0.69      0.68       635\n",
      " samples avg       0.67      0.70      0.66       635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "outputs = rnn_asp.predict(X_dev_asp)\n",
    "utils.get_reports(\n",
    "    y_true = Y_dev_asp.values, \n",
    "    y_pred= outputs\n",
    ") #non-trainable 300d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e2f55b",
   "metadata": {},
   "source": [
    "## 2.4) CNN (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55950604",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_params = dict(\n",
    "    n_filters = 64,\n",
    "    kernel_size = 3,\n",
    "    n_cnn_layers = 3,\n",
    "    dense_layers = [64,64],\n",
    "    embedding_matrix = embedding_matrix,\n",
    "    n_outputs = 1,\n",
    "    embedding_trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c034890",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_asp = aspect.dl_glove(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "447dafa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained word embedding\n",
      "cloning model from template...\n"
     ]
    }
   ],
   "source": [
    "# Reinstantiate models\n",
    "cnn_asp.set_model_template(get_cnn(**cnn_params), n_models = len(Y_TRAIN_asp.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "336a24a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting service ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 4s 19ms/step - loss: 0.5369 - accuracy: 0.7995 - val_loss: 0.4988 - val_accuracy: 0.7950\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 0.4774 - accuracy: 0.8063 - val_loss: 0.4615 - val_accuracy: 0.7950\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 0.3765 - accuracy: 0.8286 - val_loss: 0.3308 - val_accuracy: 0.8781\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.2190 - accuracy: 0.9230 - val_loss: 0.2930 - val_accuracy: 0.8917\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 1s 15ms/step - loss: 0.1061 - accuracy: 0.9714 - val_loss: 0.3829 - val_accuracy: 0.8936\n",
      "fitting food ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 4s 19ms/step - loss: 0.6731 - accuracy: 0.5821 - val_loss: 0.6228 - val_accuracy: 0.6983\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.5508 - accuracy: 0.7317 - val_loss: 0.5280 - val_accuracy: 0.7389\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.4237 - accuracy: 0.8213 - val_loss: 0.4703 - val_accuracy: 0.7756\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2970 - accuracy: 0.8862 - val_loss: 0.4520 - val_accuracy: 0.8124\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.1724 - accuracy: 0.9438 - val_loss: 0.7247 - val_accuracy: 0.7930\n",
      "fitting anecdotes/miscellaneous ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 2s 20ms/step - loss: 0.6454 - accuracy: 0.6242 - val_loss: 0.6072 - val_accuracy: 0.6306\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.5743 - accuracy: 0.6969 - val_loss: 0.5041 - val_accuracy: 0.7466\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.4422 - accuracy: 0.8019 - val_loss: 0.4640 - val_accuracy: 0.7872\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.3264 - accuracy: 0.8673 - val_loss: 0.5635 - val_accuracy: 0.7602\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2200 - accuracy: 0.9172 - val_loss: 0.6026 - val_accuracy: 0.7795\n",
      "fitting price ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 2s 20ms/step - loss: 0.3951 - accuracy: 0.8896 - val_loss: 0.3382 - val_accuracy: 0.8917\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 1s 20ms/step - loss: 0.3391 - accuracy: 0.8939 - val_loss: 0.3277 - val_accuracy: 0.8917\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 1s 21ms/step - loss: 0.2946 - accuracy: 0.8939 - val_loss: 0.3392 - val_accuracy: 0.8917\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2236 - accuracy: 0.9007 - val_loss: 0.3045 - val_accuracy: 0.8936\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 1s 16ms/step - loss: 0.1469 - accuracy: 0.9366 - val_loss: 0.4411 - val_accuracy: 0.9072\n",
      "fitting ambience ...\n",
      "\n",
      "Epoch 1/5\n",
      "65/65 [==============================] - 3s 27ms/step - loss: 0.4522 - accuracy: 0.8542 - val_loss: 0.4121 - val_accuracy: 0.8588\n",
      "Epoch 2/5\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.4031 - accuracy: 0.8571 - val_loss: 0.4132 - val_accuracy: 0.8588\n",
      "Epoch 3/5\n",
      "65/65 [==============================] - 1s 19ms/step - loss: 0.3748 - accuracy: 0.8571 - val_loss: 0.4058 - val_accuracy: 0.8588\n",
      "Epoch 4/5\n",
      "65/65 [==============================] - 1s 18ms/step - loss: 0.2975 - accuracy: 0.8649 - val_loss: 0.4166 - val_accuracy: 0.8530\n",
      "Epoch 5/5\n",
      "65/65 [==============================] - 1s 17ms/step - loss: 0.2211 - accuracy: 0.8978 - val_loss: 0.6378 - val_accuracy: 0.8569\n"
     ]
    }
   ],
   "source": [
    "histories = cnn_asp.fit(\n",
    "                X_train_asp, Y_train_asp, X_dev_asp, Y_dev_asp,\n",
    "                batch_size = 32, epochs = 5\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60895b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting service...\n",
      "predicting food...\n",
      "predicting anecdotes/miscellaneous...\n",
      "predicting price...\n",
      "predicting ambience...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.58      0.69       106\n",
      "           1       0.87      0.57      0.69       209\n",
      "           2       0.72      0.67      0.69       191\n",
      "           3       0.83      0.18      0.29        56\n",
      "           4       0.44      0.05      0.10        73\n",
      "\n",
      "   micro avg       0.79      0.51      0.62       635\n",
      "   macro avg       0.74      0.41      0.49       635\n",
      "weighted avg       0.77      0.51      0.59       635\n",
      " samples avg       0.56      0.53      0.54       635\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samsa\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def manual_predict(X, threshold=0.5):\n",
    "    outputs = []\n",
    "    threshold = 0.5\n",
    "    for aspect, model in zip(Y_train_asp.columns, cnn_asp.models):\n",
    "        print(f'predicting {aspect}...')\n",
    "        y_pred_target = model.predict(X)\n",
    "        y_pred = tf.cast(y_pred_target > threshold, tf.int32) \n",
    "        outputs.append(y_pred.numpy().ravel())\n",
    "\n",
    "    outputs = np.transpose(np.array(outputs))\n",
    "    return outputs\n",
    "\n",
    "outputs = manual_predict(X_dev_asp)\n",
    "utils.get_reports(\n",
    "    y_true = Y_dev_asp.values, \n",
    "    y_pred= outputs\n",
    ") #non-trainable 300d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3072ee0",
   "metadata": {},
   "source": [
    "# $\\text{Inference}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bdd0d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from my_models import inference\n",
    "\n",
    "sent_names = ['logreg_sent_bow','logreg_sent_tfidf','rnn_sent','cnn_sent']\n",
    "asp_names = ['logreg_aspect','rnn_asp']\n",
    "model_names = itertools.product(sent_names, asp_names)\n",
    "\n",
    "sent_models = [\n",
    "    logreg_sent_bow,\n",
    "    logreg_sent_tfidf,\n",
    "    rnn_sent,\n",
    "    cnn_sent\n",
    "]\n",
    "\n",
    "asp_models = [\n",
    "    logreg_aspect,\n",
    "    rnn_asp\n",
    "]\n",
    "models = itertools.product(sent_models, asp_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6fbc381",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('logreg_sent_bow', 'logreg_aspect')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MulBinary_logreg.preprocess() got an unexpected keyword argument 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[0;32m      6\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mInferenceModel(\u001b[38;5;241m*\u001b[39mmodel)\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43minferencer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_inference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m outputs\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting_predictions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_train-set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Downloads\\University-Chula\\Y4-2\\Contest 1 Sentiment Analysis\\my_models\\inference.py:11\u001b[0m, in \u001b[0;36mInferenceModel.predict\u001b[1;34m(self, X_raw)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_raw:pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m     10\u001b[0m     X_sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_model\u001b[38;5;241m.\u001b[39mpreprocess(X_raw\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 11\u001b[0m     X_asp  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maspect_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     sent_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_model\u001b[38;5;241m.\u001b[39mpredict(X_sent)\n\u001b[0;32m     14\u001b[0m     asp_pred  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maspect_model\u001b[38;5;241m.\u001b[39mpredict(X_asp)\n",
      "\u001b[1;31mTypeError\u001b[0m: MulBinary_logreg.preprocess() got an unexpected keyword argument 'vocab_size'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"contest1_train.csv\")\n",
    "df_train_inference = df_train[['id','text']]\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    print(name)\n",
    "    inferencer = inference.InferenceModel(*model)\n",
    "    outputs = inferencer.predict(df_train_inference)\n",
    "\n",
    "    outputs.to_csv(f\"resulting_predictions/{name[0]}_{name[1]}_train-set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988f120",
   "metadata": {},
   "source": [
    "## TEST set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdba5d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting service...\n",
      "predicting food...\n",
      "predicting anecdotes/miscellaneous...\n",
      "predicting price...\n",
      "predicting ambience...\n"
     ]
    }
   ],
   "source": [
    "from my_models import inference\n",
    "\n",
    "inferencer = inference.InferenceModel(logreg_sent_bow, rnn_asp)\n",
    "\n",
    "df_test = pd.read_csv(\"contest1_train.csv\")\n",
    "\n",
    "df_train_inference = df_test[['id','text']]\n",
    "outputs = inferencer.predict(df_train_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "423e7b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspectCategory</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>anecdotes/miscellaneous</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4483 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               aspectCategory  polarity\n",
       "id                                     \n",
       "3121                  service  negative\n",
       "2777                     food  negative\n",
       "2777                     food  negative\n",
       "1634                     food  positive\n",
       "2534                  service  positive\n",
       "...                       ...       ...\n",
       "1163                  service  positive\n",
       "216   anecdotes/miscellaneous  positive\n",
       "1109                     food  positive\n",
       "899                   service  positive\n",
       "899                      food  positive\n",
       "\n",
       "[4483 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c064f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"resulting_predictions/\"\n",
    "outputs.to_csv(path_to_save + \"train-pred-2-bow-rnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fec8f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', ',', '.', 'of', 'to', 'and', 'in', 'a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "max_len = 4  \n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " output_mode='int',\n",
    " vocabulary=vocab)\n",
    "\n",
    "\n",
    "vectorize_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64facbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 13), dtype=int64, numpy=\n",
       "array([[  902,    31,  6737,     5,  9567,    48,     9,  3760,  2820,\n",
       "            6,     9,  2831,  2856],\n",
       "       [ 3882,   210,     2,  2758,    45,    35,    49,    81, 10134,\n",
       "        27754,     0,     0,     0]], dtype=int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(['Either an array of strings or a string path to a text file', 'f set, the output will have its time dimension padded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfba8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
